# QwenAnalog ‚Äî –ê–Ω–∞–ª–æ–≥ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen 2.5 (2.5B)

–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –Ω—É–ª—è, –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤.
–¢—ã —Å–∞–º –ø–æ–¥–±–∏—Ä–∞–µ—à—å –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –æ–±—É—á–∞–µ—à—å –º–æ–¥–µ–ª—å.

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
qwen_analog/
‚îú‚îÄ‚îÄ model.py          ‚Üê –í—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ train.py          ‚Üê –°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
‚îú‚îÄ‚îÄ train_config.json ‚Üê –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
‚îî‚îÄ‚îÄ README.md
```

---

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–∞–Ω–∞–ª–æ–≥ Qwen 2.5)

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –ó–Ω–∞—á–µ–Ω–∏–µ |
|-----------|---------|
| –ü–∞—Ä–∞–º–µ—Ç—Ä—ã | ~2.5B |
| –°–ª–æ–∏ | 36 |
| Hidden size | 2048 |
| Attention heads | 16 |
| KV heads (GQA) | 8 |
| FFN size | 11008 |
| –ú–∞–∫—Å. –∫–æ–Ω—Ç–µ–∫—Å—Ç | 32768 —Ç–æ–∫–µ–Ω–æ–≤ |
| –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–∏–Ω–≥–∏ | RoPE (theta=1M) |
| –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è | RMSNorm |
| –ê–∫—Ç–∏–≤–∞—Ü–∏—è FFN | SwiGLU |
| Attention | Grouped Query Attention (GQA) |

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers datasets
```

### 2. –ü–æ–¥–≥–æ—Ç–æ–≤—å –¥–∞—Ç–∞—Å–µ—Ç

–§–æ—Ä–º–∞—Ç ‚Äî JSONL —Ñ–∞–π–ª, –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞:
```json
{"text": "–¢–≤–æ–π —Ç–µ–∫—Å—Ç –∑–¥–µ—Å—å..."}
```

### 3. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è

```bash
# –û–¥–Ω–∞ GPU
python train.py --data ./my_dataset.jsonl --config train_config.json

# –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞
python train.py --data ./my_dataset.jsonl --resume ./checkpoints/step_1000/checkpoint.pt
```

### 4. Multi-GPU (DDP)

```bash
torchrun --nproc_per_node=4 train.py --data ./my_dataset.jsonl
```

---

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

–í `train.py` –∑–∞–º–µ–Ω–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Å–≤–æ–π:

```python
# –í–∞—Ä–∏–∞–Ω—Ç 1: Qwen —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (BPE, 151936 —Ç–æ–∫–µ–Ω–æ–≤)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")

# –í–∞—Ä–∏–∞–Ω—Ç 2: SentencePiece —Å–≤–æ–π
import sentencepiece as spm
sp = spm.SentencePieceProcessor()
sp.Load("my_tokenizer.model")

# –í–∞—Ä–∏–∞–Ω—Ç 3: tiktoken
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
```

–ù–µ –∑–∞–±—É–¥—å –æ–±–Ω–æ–≤–∏—Ç—å `vocab_size` –≤ `train_config.json`!

---

## üíæ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∂–µ–ª–µ–∑—É

| –†–µ–∂–∏–º | VRAM |
|-------|------|
| Inference (fp32) | ~10GB |
| Inference (bf16) | ~5GB |
| –û–±—É—á–µ–Ω–∏–µ (batch=1, grad_ckpt) | ~24GB |
| –û–±—É—á–µ–Ω–∏–µ (batch=4) | ~80GB |

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:** A100 80GB –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ RTX 3090/4090.

---

## üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏

```python
import torch
from model import QwenForCausalLM, QwenConfig
from transformers import AutoTokenizer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
config = QwenConfig()
model = QwenForCausalLM(config)

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
ckpt = torch.load("checkpoints/final/checkpoint.pt")
model.load_state_dict(ckpt["model_state_dict"])
model.eval()

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
prompt = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
input_ids = torch.tensor([tokenizer.encode(prompt)])
output = model.generate(input_ids, max_new_tokens=100, temperature=0.7)
print(tokenizer.decode(output[0]))

# –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_parameters()/1e9:.2f}B")
```

---

## üìà –°–æ–≤–µ—Ç—ã –ø–æ –æ–±—É—á–µ–Ω–∏—é

1. **–†–∞–∑–æ–≥—Ä–µ–≤ LR** ‚Äî –¥–æ–±–∞–≤—å warmup –ø–µ—Ä–≤—ã–µ 1-2% —à–∞–≥–æ–≤
2. **Gradient checkpointing** ‚Äî –≤–∫–ª—é—á–∏ –ø—Ä–∏ –Ω–µ—Ö–≤–∞—Ç–∫–µ VRAM
3. **bf16** ‚Äî –±—ã—Å—Ç—Ä–µ–µ fp16 –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU
4. **Batch size** ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π gradient accumulation –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–æ–≥–æ –±–∞—Ç—á–∞
5. **–î–∞—Ç–∞—Å–µ—Ç** ‚Äî –º–∏–Ω–∏–º—É–º 1-10B —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM

---

## üîÑ –û—Ç–ª–∏—á–∏—è –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ Qwen 2.5

- –ù–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å HuggingFace `transformers` (–Ω–æ –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å)
- –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–±–µ–∑ beam search)
- –ù–µ—Ç Flash Attention (–¥–æ–±–∞–≤—å `flash-attn` –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)

–î–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è Flash Attention:
```python
# pip install flash-attn
from flash_attn import flash_attn_func
# –ó–∞–º–µ–Ω–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π softmax attention –≤ GQAttention.forward()
```
